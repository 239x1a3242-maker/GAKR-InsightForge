"""
Main FastAPI app for AI Data Analysis Platform - COLUMN VALIDATION FIXED
"""

from fastapi import FastAPI, UploadFile, File, Form, Request
from fastapi.responses import JSONResponse, HTMLResponse
from fastapi.staticfiles import StaticFiles
from fastapi.templating import Jinja2Templates
from fastapi.exceptions import HTTPException
from pathlib import Path
import shutil
import numpy as np
import pandas as pd
import traceback
import os

from data_service import process_dataset, DataValidationError
from ml_service import train_models, save_model
from viz_service import (
    plot_model_comparison,
    plot_actual_vs_predicted,
    plot_feature_importance,
    plot_clusters,
    plot_confusion_matrix,
    get_viz_plan,
)
from ai_insights_service import generate_structured_narrative

app = FastAPI(title="AI Data Analysis Platform")

templates = Jinja2Templates(directory="templates")

# Mount static files for charts
app.mount("/charts", StaticFiles(directory="data/temp_viz"), name="charts")

# âœ… DIRECTORIES
UPLOAD_DIR = Path("data/uploads")
UPLOAD_DIR.mkdir(parents=True, exist_ok=True)

TEMP_VIZ_DIR = Path("data/temp_viz")
TEMP_VIZ_DIR.mkdir(parents=True, exist_ok=True)

# âœ… FIXED ANALYSIS PIPELINE WITH COLUMN VALIDATION
@app.post("/upload/")
async def analyze_pipeline(
    file: UploadFile = File(None),
    target_column: str = Form(...),
):
    if not file or not file.filename or file.filename.strip() == "":
        return JSONResponse(
            content={"success": False, "error": "No file uploaded. Please select a file."},
            status_code=400,
        )
    
    if not target_column or target_column.strip() == "":
        return JSONResponse(
            content={"success": False, "error": "No target column specified."},
            status_code=400,
        )
    
    file_path = UPLOAD_DIR / file.filename
    with open(file_path, "wb") as buffer:
        shutil.copyfileobj(file.file, buffer)
    
    # âœ… STEP 0: IMMEDIATE COLUMN VALIDATION
    try:
        print(f"ðŸ“ Validating: {file.filename}")
        df_check = pd.read_csv(file_path)
        available_columns = df_check.columns.str.strip().tolist()  # Strip whitespace
        
        print(f"ðŸ“‹ Available columns: {available_columns}")
        print(f"ðŸŽ¯ Requested target: '{target_column}'")
        
        if target_column not in available_columns:
            similar_cols = [col for col in available_columns if target_column.lower() in col.lower()]
            error_msg = f"Target column '{target_column}' not found. Available: {available_columns}"
            if similar_cols:
                error_msg += f"\nðŸ’¡ Did you mean: {similar_cols}?"
            
            return JSONResponse(
                content={
                    "success": False,
                    "pipeline_status": "VALIDATION_FAILED",
                    "error": error_msg,
                    "available_columns": available_columns,
                    "suggestions": similar_cols if similar_cols else [],
                    "file_name": file.filename
                },
                status_code=400,
            )
        
        print(f"âœ… Target '{target_column}' VALIDATED âœ“")
        
    except Exception as e:
        return JSONResponse(
            content={"success": False, "error": f"File read error: {str(e)}"},
            status_code=400,
        )
    
    try:
        print(f"ðŸš€ Starting pipeline: {file.filename} â†’ {target_column}")
        
        # ========================================
        # STEP 1: DATA SERVICE (NOW WITH VALIDATION)
        # ========================================
        print("ðŸ”„ Step 1: data_service.process_dataset...")
        
        # Configure validation settings
        validation_config = {
            "missing_thresholds": {"column_max_missing_pct": 50.0, "row_max_missing_pct": 80.0},
            "outlier_method": "iqr",
            "outlier_threshold": 1.5,
            "fail_on_warnings": False  # Allow warnings but continue processing
        }
        
        try:
            data_results = process_dataset(
                file_path=str(file_path),
                target_column=target_column.strip(),  # Clean target
                generate_html_report=True,
                validation_config=validation_config,
                fail_on_validation_error=True
            )
        except DataValidationError as e:
            return JSONResponse(
                content={
                    "success": False,
                    "pipeline_status": "VALIDATION_FAILED",
                    "error": f"Data validation failed: {str(e)}",
                    "file_name": file.filename,
                    "target_column": target_column
                },
                status_code=400,
            )
        
        cleaned_df = data_results["cleaned_dataframe"]
        processed_data = data_results["processed_data"]
        profile_summary = data_results["profile_summary"]
        validation_report = data_results["validation_report"]
        
        print(f"âœ… Data processing complete. Cleaned shape: {cleaned_df.shape}")
        print(f"âœ… Validation status: {validation_report['overall_status']}")
        
        # ========================================
        # STEP 2: ML SERVICE
        # ========================================
        print("ðŸ”„ Step 2: ml_service.train_models...")
        ml_results = train_models(processed_data)
        best_model = ml_results["best_model"]
        best_model_name = ml_results["best_model_name"]
        task_type = ml_results["task"]
        leaderboard = ml_results["leaderboard"]
        feature_names = ml_results.get("feature_names", processed_data.get("feature_names", []))
        dataset_warnings = ml_results.get("dataset_warnings", [])
        
        save_model(best_model, task=task_type, model_name=best_model_name)
        
        # Predictions
        y_pred_sample = None
        y_true_sample = None
        preds_full = None
        y_test = None
        if task_type != "unsupervised":
            X_test = processed_data["X_test"]
            y_test = processed_data["y_test"]
            preds_full = best_model.predict(X_test)
            y_pred_sample = preds_full[:20].tolist()
            y_true_sample = np.array(y_test)[:20].tolist()
        
        print(f"âœ… ML complete. Best: {best_model_name} | Task: {task_type}")
        
        # ========================================
        # STEP 3: VIZ SERVICE
        # ========================================
        print("ðŸ”„ Step 3: viz_service...")
        viz_results = {}
        
        # Ensure viz dir is clean
        for old_file in TEMP_VIZ_DIR.glob("*.png"):
            old_file.unlink()
        
        model_comparison_path = TEMP_VIZ_DIR / "model_comparison.png"
        plot_model_comparison(leaderboard, save_path=str(model_comparison_path))
        viz_results["model_comparison"] = str(model_comparison_path)
        
        if best_model and hasattr(best_model, "feature_importances_") and feature_names:
            fi_path = TEMP_VIZ_DIR / "feature_importance.png"
            plot_feature_importance(best_model, feature_names, save_path=str(fi_path))
            viz_results["feature_importance"] = str(fi_path)
        elif task_type == "classification" and len(feature_names) > 0:
            # For models without built-in feature importance, try permutation importance
            try:
                from sklearn.inspection import permutation_importance
                from sklearn.dummy import DummyClassifier
                
                # Fit a simple model for feature importance if the best model doesn't have it
                dummy_model = DummyClassifier(strategy="most_frequent")
                dummy_model.fit(processed_data["X_train"], processed_data["y_train"])
                
                # Calculate permutation importance
                perm_importance = permutation_importance(best_model, processed_data["X_test"], processed_data["y_test"], n_repeats=10, random_state=42)
                
                # Plot permutation importance
                fi_path = TEMP_VIZ_DIR / "feature_importance.png"
                importances = pd.Series(perm_importance.importances_mean, index=feature_names).sort_values(ascending=False).head(10)
                plt.figure(figsize=(8, 6))
                sns.barplot(x=importances.values, y=importances.index, orient="h")
                plt.title("Feature Importance (Permutation)")
                plt.xlabel("Importance")
                plt.ylabel("Feature")
                plt.tight_layout()
                plt.savefig(str(fi_path), bbox_inches="tight")
                plt.close()
                viz_results["feature_importance"] = str(fi_path)
            except Exception as e:
                print(f"Could not calculate feature importance: {e}")
        
        if task_type != "unsupervised" and preds_full is not None and y_test is not None:
            avp_path = TEMP_VIZ_DIR / "actual_vs_predicted.png"
            plot_actual_vs_predicted(y_test, preds_full, save_path=str(avp_path))
            viz_results["actual_vs_predicted"] = str(avp_path)
            
            # Add confusion matrix for classification
            if task_type == "classification" and best_model_name in ml_results["metrics"]:
                metrics = ml_results["metrics"][best_model_name]
                if "Confusion_Matrix" in metrics:
                    cm_path = TEMP_VIZ_DIR / "confusion_matrix.png"
                    # Get class names from the target column
                    class_names = None
                    if hasattr(processed_data.get("y_train"), 'unique'):
                        class_names = sorted(processed_data["y_train"].unique().tolist())
                    plot_confusion_matrix(metrics["Confusion_Matrix"], class_names=class_names, save_path=str(cm_path))
                    viz_results["confusion_matrix"] = str(cm_path)
        
        if task_type == "unsupervised":
            X_all = processed_data["X_processed"]
            labels = best_model.fit_predict(X_all)
            cluster_path = TEMP_VIZ_DIR / "cluster_plot.png"
            plot_clusters(X_all, labels, save_path=str(cluster_path))
            viz_results["cluster_plot"] = str(cluster_path)
        
        viz_plan = get_viz_plan(cleaned_df, target_column, ml_task=task_type)
        viz_results["recommendations"] = viz_plan
        viz_results["chart_types"] = viz_plan
        
        print("âœ… Viz service complete.")
        
        # ========================================
        # STEP 4: AI SERVICE
        # ========================================
        print("ðŸ”„ Step 4: ai_service...")
        """
        analysis_payload = {
            "dataset_summary": {
                "rows": profile_summary.get("rows"),
                "columns": profile_summary.get("columns"),
                "memory_mb": profile_summary.get("memory_mb"),
            },
            "data_quality": {
                "missing_pct": profile_summary.get("missing_pct"),
                "duplicates": profile_summary.get("duplicates"),
            },
            "model_info": {"name": best_model_name, "task": task_type},
            "model_metrics": ml_results["metrics"],
            "feature_importance": (
                dict(zip(feature_names, best_model.feature_importances_))
                if best_model and hasattr(best_model, "feature_importances_") and feature_names
                else {}
            ),
            "predictions_summary": {
                "y_true_sample": y_true_sample,
                "y_pred_sample": y_pred_sample,
            },
            "charts": viz_results["chart_types"],
        }
        
        structured_narrative = generate_structured_narrative(analysis_payload)
        """
        print("âœ… AI insights generation complete.")
        structured_narrative = {"narrative": "Structured narrative generation is currently disabled."}
        print("âœ… Pipeline COMPLETE!")
        
        # ========================================
        # FINAL FRONTEND RESPONSE
        # ========================================
        final_response = {
            "success": True,
            "pipeline_status": "COMPLETE",
            "file_name": file.filename,
            "target_column": target_column,
            "available_columns": available_columns,  # For frontend
            "data_results": {
                "profile_summary": profile_summary,
                "validation_report": validation_report,
                "cleaned_shape": list(cleaned_df.shape),
                "eda_report_path": data_results.get("eda_report_path"),
            },
            "ml_results": {
                "task": task_type,
                "best_model_name": best_model_name,
                "leaderboard": leaderboard,
                "metrics": ml_results["metrics"],
                "dataset_warnings": dataset_warnings,
                "y_true_sample": y_true_sample,
                "y_pred_sample": y_pred_sample,
            },
            "viz_results": {
                "charts": viz_results,
                "chart_paths": {k: f"/charts/{Path(v).name}" for k, v in viz_results.items() if isinstance(v, str) and v.endswith('.png')}
            },
            "ai_results": {
                "structured_narrative": structured_narrative,
            },
        }
        
        return JSONResponse(content=final_response)
    
    except Exception as e:
        error_msg = f"Pipeline failed: {str(e)}"
        traceback.print_exc()
        return JSONResponse(
            content={
                "success": False, 
                "pipeline_status": "FAILED",
                "error": error_msg,
                "available_columns": available_columns if 'available_columns' in locals() else []
            },
            status_code=500,
        )
    finally:
        if file_path.exists():
            file_path.unlink()

# âœ… HOMEPAGE & HEALTH (UNCHANGED)
@app.get("/", response_class=HTMLResponse)
async def home(request: Request):
    try:
        return templates.TemplateResponse("index.html", {"request": request})
    except:
        return HTMLResponse("<h1>AI Data Analysis Platform</h1><p>Frontend not found.</p>")

@app.get("/health")
async def health_check():
    return {"status": "healthy"}

# âœ… FIXED SERVER START
if __name__ == "__main__":
    import uvicorn
    print("\n" + "=" * 70)
    print("ðŸš€ AI DATA ANALYSIS PLATFORM - COLUMN VALIDATION FIXED")
    print("=" * 70)
    print("ðŸ†• NEW: POST /validate-columns/ - Check columns before analysis!")
    print("=" * 70)
    uvicorn.run(
        "__main__:app", 
        host="0.0.0.0", 
        port=8080, 
        reload=True,
        log_level="info"
    )
